{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec616fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def csr_info(mat, name=\"\", non_empy=False):\n",
    "    r\"\"\" Print out info about this CSR matrix. If non_empy, \n",
    "    report number of non-empty rows and cols as well\n",
    "    \"\"\"\n",
    "    if non_empy:\n",
    "        print(\"%s [nrows %d (%d non-empty), ncols %d (%d non-empty), nnz %d]\" % (\n",
    "                name, mat.shape[0], \n",
    "                sum(1 if mat.indptr[i+1] > mat.indptr[i] else 0 \n",
    "                for i in range(mat.shape[0])), \n",
    "                mat.shape[1], len(np.unique(mat.indices)), \n",
    "                len(mat.data)))\n",
    "    else:\n",
    "        print( \"%s [nrows %d, ncols %d, nnz %d]\" % (name, \n",
    "                mat.shape[0], mat.shape[1], len(mat.data)) )\n",
    "        \n",
    "# # scale matrix and normalize its rows\n",
    "# def csr_idf(mat, copy=False, **kargs):\n",
    "#     r\"\"\" Scale a CSR matrix by idf. \n",
    "#     Returns scaling factors as dict. If copy is True, \n",
    "#     returns scaled matrix and scaling factors.\n",
    "#     \"\"\"\n",
    "#     if copy is True:\n",
    "#         mat = mat.copy()\n",
    "#     nrows = mat.shape[0]\n",
    "#     nnz = mat.nnz\n",
    "#     ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "#     # document frequency\n",
    "#     df = defaultdict(int)\n",
    "#     for i in ind:\n",
    "#         df[i] += 1\n",
    "#     # inverse document frequency\n",
    "#     for k,v in df.items():\n",
    "#         df[k] = np.log(nrows / float(v))  ## df turns to idf - reusing memory\n",
    "#     # scale by idf\n",
    "#     for i in range(0, nnz):\n",
    "#         val[i] *= df[ind[i]]\n",
    "        \n",
    "#     return df if copy is False else mat\n",
    "\n",
    "# def csr_l2normalize(mat, copy=False, **kargs):\n",
    "#     r\"\"\" Normalize the rows of a CSR matrix by their L-2 norm. \n",
    "#     If copy is True, returns a copy of the normalized matrix.\n",
    "#     \"\"\"\n",
    "#     if copy is True:\n",
    "#         mat = mat.copy()\n",
    "#     nrows = mat.shape[0]\n",
    "#     nnz = mat.nnz\n",
    "#     ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "#     # normalize\n",
    "#     for i in range(nrows):\n",
    "#         rsum = 0.0    \n",
    "#         for j in range(ptr[i], ptr[i+1]):\n",
    "#             rsum += val[j]**2\n",
    "#         if rsum == 0.0:\n",
    "#             continue  # do not normalize empty rows\n",
    "#         rsum = 1.0/np.sqrt(rsum)\n",
    "#         for j in range(ptr[i], ptr[i+1]):\n",
    "#             val[j] *= rsum\n",
    "            \n",
    "#     if copy is True:\n",
    "#         return mat\n",
    "\n",
    "def build_matrix_1(docs, labels):\n",
    "    r\"\"\" Build sparse matrix from a list of documents, \n",
    "    each of which is a list of word/terms in the document.  \n",
    "    \"\"\"\n",
    "        \n",
    "    distinctWordsAndIndex = {}\n",
    "    indexIter = 0\n",
    "    nnz = 0\n",
    "    \n",
    "    for idx, doc in enumerate(docs):\n",
    "        frequency = doc.split()\n",
    "        while frequency:\n",
    "            term, freq, *frequency = frequency\n",
    "#             print(term)\n",
    "            if term not in labels:\n",
    "                continue\n",
    "            \n",
    "            nnz += 1\n",
    "            if term not in distinctWordsAndIndex:\n",
    "                distinctWordsAndIndex[term] = indexIter\n",
    "                indexIter += 1    \n",
    "    nrows = len(docs)\n",
    "    ncols = len(distinctWordsAndIndex)\n",
    "    \n",
    "    # set up memory\n",
    "    ind = np.zeros(nnz, dtype=int)\n",
    "    val = np.zeros(nnz, dtype=int)\n",
    "    ptr = np.zeros(nrows+1, dtype=int)\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for idx, doc in enumerate(docs):\n",
    "        ptr[j] = i\n",
    "        j += 1\n",
    "        frequency = doc.split()\n",
    "        while frequency:\n",
    "            term, freq, *frequency = frequency\n",
    "            if term not in labels:\n",
    "                continue\n",
    "            ind[i] = distinctWordsAndIndex[term]\n",
    "            val[i] = int(freq)\n",
    "            i += 1\n",
    "    ptr[j] = i\n",
    "\n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def BisectingKMeans(mat4, k_start, k_end, step):\n",
    "    for k in range(k_start, k_end+1, step):\n",
    "#         print('============================================')\n",
    "        X = mat4\n",
    "        num_clusters = k\n",
    "        k_list = [] \n",
    "        sse_list = [] \n",
    "        total_SSE = 0\n",
    "        current_clusters = 1\n",
    "\n",
    "        clusterMap = {}\n",
    "        for idx,row in enumerate(X):\n",
    "            clusterMap[idx] = idx\n",
    "        finalClusterLabels = {}\n",
    "\n",
    "        while current_clusters != num_clusters:\n",
    "        #     print('final labels', finalClusterLabels)\n",
    "        #     print('clusterMap',clusterMap)\n",
    "            kmeans = KMeans(n_clusters=2).fit(X)\n",
    "    #         print(kmeans.inertia_ )\n",
    "            cluster_centers = kmeans.cluster_centers_\n",
    "        #     print(X.shape)\n",
    "            sse = [0]*2\n",
    "            for point, label in zip(X, kmeans.labels_):\n",
    "                sse[label] += np.square(point-cluster_centers[label]).sum()\n",
    "            chosen_cluster = np.argmax(sse, axis=0)\n",
    "            total_SSE += sse[np.argmin(sse, axis=0)]\n",
    "    #         print('SSE', sse)\n",
    "    #         print('Total SSE', total_SSE)\n",
    "    #         print('chosen_cluster', chosen_cluster)\n",
    "    #         print('kmeans labels', kmeans.labels_)\n",
    "        #     print('cluster_centers', cluster_centers.shape)\n",
    "            chosen_cluster_data = X[kmeans.labels_ == chosen_cluster]\n",
    "    # \n",
    "            newClusterMap = {}\n",
    "            clusterIter = 0\n",
    "            for idx, x in enumerate(kmeans.labels_):\n",
    "                if(x != chosen_cluster):\n",
    "                    finalClusterLabels[clusterMap[idx]] = current_clusters\n",
    "                elif current_clusters + 1 == num_clusters:\n",
    "                    finalClusterLabels[clusterMap[idx]] = current_clusters + 1\n",
    "                else:\n",
    "                    newClusterMap[clusterIter] = clusterMap[idx]\n",
    "                    clusterIter += 1 \n",
    "            clusterMap = newClusterMap\n",
    "            current_clusters += 1\n",
    "\n",
    "    #         print('chosen_cluster_data', chosen_cluster_data.shape)\n",
    "            assigned_cluster_data = X[kmeans.labels_ != chosen_cluster]\n",
    "    #         print('assigned_cluster_data', assigned_cluster_data.shape)\n",
    "            X = chosen_cluster_data\n",
    "    #         print('finalClusterLabels - len ', len(finalClusterLabels))\n",
    "\n",
    "        k_list.append(k)\n",
    "        sse_list.append(kmeans.inertia_ )\n",
    "        print_internal_metrics(mat4, finalClusterLabels)\n",
    "\n",
    "    return finalClusterLabels\n",
    "#         print(k_list, sse_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "791f6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Filter labels with length <= 3 and is present in stop words\n",
    "stop_words = stopwords.words('english')\n",
    "with open(\"train.clabel\", \"r\", encoding=\"utf8\") as fh:\n",
    "    labels = {}\n",
    "    for idx, word in enumerate(fh.readlines()):\n",
    "        if len(word.rstrip()) < 4 or word.rstrip() in stop_words:\n",
    "            continue\n",
    "        labels[str(idx+1)] = word.rstrip()\n",
    "# print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d59a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Internal Metrics\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "def print_internal_metrics(mat, labels_dict):\n",
    "    labels = [labels_dict[key] for key in sorted(labels_dict.keys())]\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    print(n_clusters_, metrics.silhouette_score(mat, labels), metrics.calinski_harabasz_score(mat, labels))\n",
    "#     print('Estimated number of clusters: %d' % n_clusters_)\n",
    "#     print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(mat, labels))\n",
    "#     print(\"Calinski Harabasz Score: %0.3f\" % metrics.calinski_harabasz_score(mat, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6543eb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before SVD\n",
      " [nrows 8580, ncols 26237, nnz 962848]\n",
      "============================================\n",
      "SVD number of concepts =  2500\n",
      "Variance ratio sum 0.8602022791689714\n",
      " [nrows 8580, ncols 2500, nnz 8580]\n",
      "2 0.0143233753608746 112.85584729817276\n",
      "3 0.01237221748789725 103.91492693842189\n",
      "4 0.01589639371147272 99.71498155205117\n",
      "5 0.019413110561624078 94.64872408364963\n",
      "6 0.016266967036870483 71.64774763398135\n",
      "7 0.020113921522872724 75.09392170418438\n",
      "8 0.020985268332499675 68.35690305718394\n",
      "9 0.0203775212836564 62.42034132224777\n",
      "10 0.022441537706512617 57.18207807332727\n",
      "11 0.02365966376741684 54.0708774009632\n",
      "12 0.023589696551001224 50.419848469820906\n",
      "13 0.020775888913758304 45.91808332990942\n",
      "14 0.02212871632163708 44.1897595767872\n",
      "15 0.02201763499010948 40.82469811861065\n",
      "16 0.021513179299882387 39.265870399576826\n",
      "17 0.02188707606867325 36.835373434563564\n",
      "18 0.01707921694479305 29.15608853105772\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_65852/2584655006.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#     print('Shape after SVD')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mcsr_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mfinalClusterLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBisectingKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mcomponents\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_65852/2085068559.py\u001b[0m in \u001b[0;36mBisectingKMeans\u001b[0;34m(mat4, k_start, k_end, step)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m#     print('final labels', finalClusterLabels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m#     print('clusterMap',clusterMap)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;31m#         print(kmeans.inertia_ )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mcluster_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0;31m# run a k-means once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m             labels, inertia, centers, n_iter_ = kmeans_single(\n\u001b[0m\u001b[1;32m   1189\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[0;34m(X, sample_weight, centers_init, max_iter, verbose, x_squared_norms, tol, n_threads)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0m_inertia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_inertia_dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m     \u001b[0minit_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter_half_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_bounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0mstrict_convergence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "with open(\"train.dat\", \"r\", encoding=\"utf8\") as fh:\n",
    "    rows = fh.readlines()\n",
    "\n",
    "mat1 = build_matrix_1(rows, labels)\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tf_idf_vector=tfidf_transformer.fit(mat1).transform(mat1)\n",
    "print('Shape before SVD')\n",
    "csr_info(tf_idf_vector)\n",
    "\n",
    "print(\"Start Time =\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "components = 2500\n",
    "while components < 2501:\n",
    "    print('============================================')\n",
    "    print('SVD number of concepts = ', components)\n",
    "    tsvd = TruncatedSVD(n_components=components)\n",
    "    mat4 = tsvd.fit(tf_idf_vector).transform(tf_idf_vector)\n",
    "    print('Variance ratio sum', tsvd.explained_variance_ratio_.sum())\n",
    "#     print(''tsvd.singular_values_.sum())\n",
    "#     print('Shape after SVD')\n",
    "    csr_info(mat4)\n",
    "    finalClusterLabels = BisectingKMeans(mat4, 10, 10, 1)\n",
    "    components += 500\n",
    "print(\"End Time =\", datetime.now().strftime(\"%H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5befc2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(k_list, sse_list)\n",
    "# plt.ylabel('SSE')\n",
    "# plt.xlabel('k')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbec488",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = [finalClusterLabels[key] for key in sorted(finalClusterLabels.keys())]\n",
    "with open(\"output.dat\", \"w\", encoding=\"utf8\") as file:\n",
    "     for item in labels:\n",
    "        file.write(\"%s\\n\" % str(item))\n",
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb05127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_labels = {}\n",
    "for label in labels:\n",
    "    \n",
    "    if label not in count_labels:\n",
    "        count_labels[label] = 1\n",
    "    else:\n",
    "        count_labels[label] = int(count_labels[label]) + 1\n",
    "#     print(count_labels[label])\n",
    "print(count_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
