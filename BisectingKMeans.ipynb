{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec616fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_matrix_1(docs, labels):\n",
    "    distinctWordsAndIndex = {}\n",
    "    indexIter = 0\n",
    "    nnz = 0\n",
    "#     print(docs[0], labels)\n",
    "    \n",
    "    for idx, doc in enumerate(docs):\n",
    "        frequency = doc.split()\n",
    "        while frequency:\n",
    "            term, freq, *frequency = frequency\n",
    "            if term not in labels:\n",
    "                continue\n",
    "            \n",
    "            nnz += 1\n",
    "            if term not in distinctWordsAndIndex:\n",
    "                distinctWordsAndIndex[term] = indexIter\n",
    "                indexIter += 1    \n",
    "                \n",
    "    nrows = len(docs)\n",
    "    ncols = len(distinctWordsAndIndex)\n",
    "    \n",
    "    ind = np.zeros(nnz, dtype=int)\n",
    "    val = np.zeros(nnz, dtype=int)\n",
    "    ptr = np.zeros(nrows+1, dtype=int)\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for idx, doc in enumerate(docs):\n",
    "        ptr[j] = i\n",
    "        j += 1\n",
    "        frequency = doc.split()\n",
    "        while frequency:\n",
    "            term, freq, *frequency = frequency\n",
    "            if term not in labels:\n",
    "                continue\n",
    "            ind[i] = distinctWordsAndIndex[term]\n",
    "            val[i] = int(freq)\n",
    "            i += 1\n",
    "    ptr[j] = i\n",
    "\n",
    "    mat = csr_matrix((val, ind, ptr), shape=(nrows, ncols), dtype=np.double)\n",
    "    mat.sort_indices()\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdbadf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing\n",
    "#Filter labels with length <= 3 and is present in stop words collection\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def getValidWords():\n",
    "    stop_words = stopwords.words('english')\n",
    "    with open(\"train.clabel\", \"r\", encoding=\"utf8\") as fh:\n",
    "        labels = {}\n",
    "#         print(len(fh.readlines()))\n",
    "        for idx, word in enumerate(fh.readlines()):\n",
    "            if len(word.rstrip()) < 4 or word.rstrip() in stop_words:\n",
    "                continue\n",
    "            labels[str(idx+1)] = word.rstrip()\n",
    "    return labels\n",
    "    # print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BisectingKMeans(mat4, k_start, k_end, step):\n",
    "    kValues = list()\n",
    "    scores = list()\n",
    "    \n",
    "    for k in range(k_start, k_end+1, step):\n",
    "        print('--------------------------------------------')\n",
    "        X = mat4\n",
    "        k_list = [] \n",
    "        sse_list = [] \n",
    "        total_SSE = 0\n",
    "        current_clusters = 1\n",
    "\n",
    "        clusterMap = {}\n",
    "        for idx,row in enumerate(X):\n",
    "            clusterMap[idx] = idx\n",
    "        finalClusterLabels = {}\n",
    "\n",
    "        while current_clusters != k:\n",
    "            kmeans = KMeans(n_clusters=2, n_init=50).fit(X)\n",
    "            cluster_centers = kmeans.cluster_centers_\n",
    "            sse = [0]*2\n",
    "            for point, label in zip(X, kmeans.labels_):\n",
    "                sse[label] += np.square(point-cluster_centers[label]).sum()\n",
    "            chosen_cluster = np.argmax(sse, axis=0)\n",
    "            total_SSE += sse[np.argmin(sse, axis=0)]\n",
    "            chosen_cluster_data = X[kmeans.labels_ == chosen_cluster]\n",
    "            newClusterMap = {}\n",
    "            clusterIter = 0\n",
    "            #to keep track of the index of the clusters\n",
    "            for idx, x in enumerate(kmeans.labels_):\n",
    "                if(x != chosen_cluster):\n",
    "                    finalClusterLabels[clusterMap[idx]] = current_clusters\n",
    "                elif current_clusters + 1 == k:\n",
    "                    finalClusterLabels[clusterMap[idx]] = current_clusters + 1\n",
    "                else:\n",
    "                    newClusterMap[clusterIter] = clusterMap[idx]\n",
    "                    clusterIter += 1 \n",
    "            clusterMap = newClusterMap\n",
    "            current_clusters += 1\n",
    "            assigned_cluster_data = X[kmeans.labels_ != chosen_cluster]\n",
    "            X = chosen_cluster_data\n",
    "        k_list.append(k)\n",
    "        sse_list.append(kmeans.inertia_ )\n",
    "        print_internal_metrics(mat4, finalClusterLabels)\n",
    "        print('--------------------------------------------')\n",
    "        \n",
    "        labels = [finalClusterLabels[key] for key in sorted(finalClusterLabels.keys())]\n",
    "        chscore = metrics.calinski_harabasz_score(mat4, labels)\n",
    "        print(chscore)\n",
    "        kValues.append(k)\n",
    "        scores.append(chscore)\n",
    "        \n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.plot(kValues, scores)\n",
    "    plt.xticks(kValues, kValues)\n",
    "    plt.xlabel('No. of Clusters (k)')\n",
    "    plt.ylabel('Calinski Harabaz Score')\n",
    "    plt.title('CH vs k score')\n",
    "\n",
    "    plt.savefig('plot.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return finalClusterLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791f6d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Internal Metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "def print_internal_metrics(mat, labels_dict):\n",
    "    labels = [labels_dict[key] for key in sorted(labels_dict.keys())]\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    print('K: %d' % n_clusters_)\n",
    "    print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(mat, labels))\n",
    "    print(\"Calinski Harabasz Score: %0.3f\" % metrics.calinski_harabasz_score(mat, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6543eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from datetime import datetime\n",
    "\n",
    "#Read the file\n",
    "with open(\"train.dat\", \"r\", encoding=\"utf8\") as fh:\n",
    "    rows = fh.readlines()\n",
    "    \n",
    "#Select valid words\n",
    "valid_words = getValidWords()\n",
    "\n",
    "#build csr matrix from valid words only\n",
    "mat1 = build_matrix_1(rows, valid_words)\n",
    "print(mat1.shape)\n",
    "\n",
    "# TF-IDF transform to normalise the matrix\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tf_idf_vector=tfidf_transformer.fit(mat1).transform(mat1)\n",
    "\n",
    "#print CST information\n",
    "print('Shape before SVD', tf_idf_vector.shape)\n",
    "\n",
    "print(\"Start Time =\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "components = 100\n",
    "while components < 101:\n",
    "    print('============================================')\n",
    "    print('SVD number of concepts = ', components)\n",
    "    tsvd = TruncatedSVD(n_components=components)\n",
    "    mat4 = tsvd.fit(tf_idf_vector).transform(tf_idf_vector)\n",
    "    print('Variance ratio sum', tsvd.explained_variance_ratio_.sum())\n",
    "    finalClusterLabels = BisectingKMeans(mat4, 3, 23, 2)\n",
    "    components += 50\n",
    "    print('============================================')\n",
    "    \n",
    "print(\"End Time =\", datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot k vs SSE graph for k-means\n",
    "# components = 200\n",
    "# while components < 501:\n",
    "#     print('concepts', components)\n",
    "#     wcss = []\n",
    "#     for i in range(2,23,2):\n",
    "#         kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0)\n",
    "#         kmeans.fit(tf_idf_vector)\n",
    "#         wcss.append(kmeans.inertia_)\n",
    "#         print('k', i)\n",
    "#     plt.plot(range(2,23,2),wcss)\n",
    "#     plt.title('The Elbow Method')\n",
    "#     plt.xlabel('Number of clusters')\n",
    "#     plt.ylabel('WCSS')\n",
    "#     plt.savefig('elbow.png')\n",
    "#     plt.show()\n",
    "#     components += 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bbd24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot components vs Variance ratio for SVD\n",
    "components = 50\n",
    "wcss = []\n",
    "num = [] \n",
    "while components < 501:\n",
    "    \n",
    "    tsvd = TruncatedSVD(n_components=components)\n",
    "    mat4 = tsvd.fit(tf_idf_vector).transform(tf_idf_vector)\n",
    "    wcss.append(tsvd.explained_variance_ratio_.sum())\n",
    "    num.append(components)\n",
    "    components += 50\n",
    "    \n",
    "plt.plot(num, wcss)\n",
    "plt.title('Variance ratio vs Number of components')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Variance ratio')\n",
    "plt.savefig('variance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbec488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the cluster info to output file\n",
    "labels = [finalClusterLabels[key] for key in sorted(finalClusterLabels.keys())]\n",
    "with open(\"output.dat\", \"w\", encoding=\"utf8\") as file:\n",
    "     for item in labels:\n",
    "        file.write(\"%s\\n\" % str(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the number of elements in each cluster\n",
    "count_labels = {}\n",
    "for label in labels:\n",
    "    \n",
    "    if label not in count_labels:\n",
    "        count_labels[label] = 1\n",
    "    else:\n",
    "        count_labels[label] = int(count_labels[label]) + 1\n",
    "print(count_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
